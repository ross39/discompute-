#!/bin/bash

# Discompute Distributed MNIST Training Demo
# This script demonstrates distributed neural network training across iOS devices

set -e

echo "ğŸš€ Discompute Distributed MNIST Training Demo"
echo "=============================================="

# Check if Go is installed
if ! command -v go &> /dev/null; then
    echo "âŒ Go is not installed. Please install Go 1.19+ first."
    exit 1
fi

# Build the discompute binary
echo "ğŸ”¨ Building discompute binary..."
cd "$(dirname "$0")/.."
go build -o bin/discompute cmd/discompute/main.go

if [ ! -f "bin/discompute" ]; then
    echo "âŒ Failed to build discompute binary"
    exit 1
fi

echo "âœ… Discompute binary built successfully"

# Check for Mac worker
MAC_WORKER="mac-worker/discompute_mac_worker.py"
if [ ! -f "$MAC_WORKER" ]; then
    echo "âŒ Mac worker not found: $MAC_WORKER"
    exit 1
fi

echo "ğŸ’» Mac worker found: $MAC_WORKER"

echo ""
echo "ğŸ“‹ Setup Instructions:"
echo "======================"
echo ""
echo "1. ğŸ Mac Device Setup:"
echo "   â€¢ Install Python 3.8+ on each Mac"
echo "   â€¢ Install ML frameworks (MLX recommended for Apple Silicon):"
echo "     - pip install mlx tinygrad torch psutil numpy"
echo "   â€¢ Copy mac-worker/discompute_mac_worker.py to each Mac"
echo ""
echo "2. ğŸŒ Network Setup:"
echo "   â€¢ Ensure all Macs are on the same Wi-Fi network"
echo "   â€¢ Make sure UDP port 5005 is not blocked"
echo "   â€¢ Mac devices will broadcast on this port"
echo ""
echo "3. ğŸš€ Running the Demo:"
echo "   â€¢ First: Start the Mac worker on each device"
echo "   â€¢ Then: Run this training demo on your coordinator Mac"
echo ""

read -p "ğŸ’» Have you started the Mac workers? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo ""
    echo "ğŸ”§ To start the Mac worker:"
    echo "   1. Open Terminal on each Mac"
    echo "   2. Run: python mac-worker/discompute_mac_worker.py"
    echo "   3. The worker will start broadcasting and wait for training jobs"
    echo ""
    echo "ğŸ’» Example output on Mac:"
    echo "   ğŸš€ Discompute Mac Worker Client"
    echo "   âœ… Mac worker running"
    echo "   ğŸ“Š Device: MacBook Pro (Apple M3 Max)"
    echo "   ğŸ“¡ Broadcasting on UDP port 5005"
    echo "   ğŸ”— HTTP server on port 8080"
    echo "   ğŸ’¡ Ready for distributed training!"
    echo ""
    exit 0
fi

echo ""
echo "ğŸ” Starting device discovery and training..."
echo ""

# Start the distributed training
./bin/discompute \
    -mode=training \
    -training-model=mnist_cnn \
    -training-epochs=5 \
    -training-batch=32 \
    -training-lr=0.001 \
    -max-devices=4 \
    -log-level=info

echo ""
echo "ğŸ‰ Demo completed!"
echo ""
echo "ğŸ“Š What happened:"
echo "â€¢ Discovered Mac devices on the network"
echo "â€¢ Distributed MNIST training across Macs"
echo "â€¢ Each Mac trained on different data batches"
echo "â€¢ Gradients were aggregated for model updates"
echo "â€¢ Training completed with distributed neural network!"
echo ""
echo "ğŸ”§ Troubleshooting:"
echo "â€¢ No devices found? Check Wi-Fi connection and Mac worker status"
echo "â€¢ Training failed? Check Mac device logs and available memory"
echo "â€¢ Network issues? Ensure UDP port 5005 is open"
echo ""
echo "ğŸ“š Next Steps:"
echo "â€¢ Scale to more Mac devices for faster training"
echo "â€¢ Try different models and datasets"
echo "â€¢ Implement model parallel training for larger models"
echo "â€¢ Add real-time monitoring and visualization"
echo "â€¢ Optimize for specific Apple Silicon chips (M1/M2/M3)"

#!/bin/bash

# Discompute Distributed MNIST Training Demo
# This script demonstrates distributed neural network training across iOS devices

set -e

echo "ğŸš€ Discompute Distributed MNIST Training Demo"
echo "=============================================="

# Check if Go is installed
if ! command -v go &> /dev/null; then
    echo "âŒ Go is not installed. Please install Go 1.19+ first."
    exit 1
fi

# Build the discompute binary
echo "ğŸ”¨ Building discompute binary..."
cd "$(dirname "$0")/.."
go build -o bin/discompute cmd/discompute/main.go

if [ ! -f "bin/discompute" ]; then
    echo "âŒ Failed to build discompute binary"
    exit 1
fi

echo "âœ… Discompute binary built successfully"

# Check for iOS client
IOS_CLIENT="ios-client/discompute_ios_enhanced.py"
if [ ! -f "$IOS_CLIENT" ]; then
    echo "âŒ iOS client not found: $IOS_CLIENT"
    exit 1
fi

echo "ğŸ“± iOS client found: $IOS_CLIENT"

echo ""
echo "ğŸ“‹ Setup Instructions:"
echo "======================"
echo ""
echo "1. ğŸ iOS Device Setup:"
echo "   â€¢ Install Pythonista 3 from the App Store (recommended)"
echo "   â€¢ Or install a-Shell or Pyto for Python support"
echo "   â€¢ Copy ios-client/discompute_ios_enhanced.py to your iOS device"
echo "   â€¢ Install required packages:"
echo "     - pip install tinygrad numpy psutil"
echo ""
echo "2. ğŸŒ Network Setup:"
echo "   â€¢ Ensure all devices are on the same Wi-Fi network"
echo "   â€¢ Make sure UDP port 5005 is not blocked"
echo "   â€¢ iOS devices will broadcast on this port"
echo ""
echo "3. ğŸš€ Running the Demo:"
echo "   â€¢ First: Start the iOS client on each device"
echo "   â€¢ Then: Run this training demo on your computer"
echo ""

read -p "ğŸ“± Have you started the iOS clients? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo ""
    echo "ğŸ”§ To start the iOS client:"
    echo "   1. Open Pythonista on your iOS device"
    echo "   2. Copy and run: discompute_ios_enhanced.py"
    echo "   3. The client will start broadcasting and wait for training jobs"
    echo ""
    echo "ğŸ“± Example output on iOS:"
    echo "   ğŸš€ Starting Enhanced Discompute iOS Client"
    echo "   âœ… iOS client running on device: iPad Pro"
    echo "   ğŸ“¡ Broadcasting on UDP port 5005"
    echo "   ğŸ”— HTTP training server on port 8080"
    echo "   ğŸ’¡ Ready for distributed training!"
    echo ""
    exit 0
fi

echo ""
echo "ğŸ” Starting device discovery and training..."
echo ""

# Start the distributed training
./bin/discompute \
    -mode=training \
    -training-model=mnist_cnn \
    -training-epochs=5 \
    -training-batch=32 \
    -training-lr=0.001 \
    -max-devices=4 \
    -log-level=info

echo ""
echo "ğŸ‰ Demo completed!"
echo ""
echo "ğŸ“Š What happened:"
echo "â€¢ Discovered iOS devices on the network"
echo "â€¢ Distributed MNIST training across devices"
echo "â€¢ Each device trained on different data batches"
echo "â€¢ Gradients were aggregated for model updates"
echo "â€¢ Training completed with distributed neural network!"
echo ""
echo "ğŸ”§ Troubleshooting:"
echo "â€¢ No devices found? Check Wi-Fi connection and iOS client status"
echo "â€¢ Training failed? Check iOS device logs and battery levels"
echo "â€¢ Network issues? Ensure UDP port 5005 is open"
echo ""
echo "ğŸ“š Next Steps:"
echo "â€¢ Scale to more iOS devices for faster training"
echo "â€¢ Try different models and datasets"
echo "â€¢ Implement model parallel training for larger models"
echo "â€¢ Add real-time monitoring and visualization"
